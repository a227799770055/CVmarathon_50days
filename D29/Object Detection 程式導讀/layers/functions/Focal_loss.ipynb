{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss_Simple(torch.nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, log_pred_prob_onehot, target):\n",
    "        ##接在log_softmax後面，利用exp把probability 轉回來\n",
    "        pred_prob_oh = torch.exp(log_pred_prob_onehot)\n",
    "        ##只要抓Label的那個值\n",
    "        pt = Variable(pred_prob_oh.data.gather(1, target.data.view(-1, 1)), requires_grad=True)\n",
    "        ##Focal Loss加入\n",
    "        modulator = (1 - pt) ** self.gamma\n",
    "        ##計算Focal_loss\n",
    "        mce = modulator * (-torch.log(pt))\n",
    "        ##返回Batch計算值\n",
    "        return mce.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chening/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3907, 0.2894, 0.3199]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor([1,0.7,0.8]).reshape(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chening/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3907, 0.2894, 0.3199]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(F.log_softmax(torch.tensor([1,0.7,0.8]).reshape(1,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chening/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "input_=F.log_softmax(torch.tensor([0.4,0.1,0.8]).reshape(1,3))\n",
    "target=torch.tensor([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1733, -1.4733, -0.7733]], dtype=torch.float64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Focal_loss=FocalLoss_Simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2243, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Focal_loss(input_,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "直接使用API计算softmax交叉熵：\n",
      "[1.4143689 1.6642545]\n",
      "\n",
      "\n",
      "利用原理计算softmax交叉熵：\n",
      "[1.4143689 1.6642545]\n",
      "\n",
      "\n",
      "[[-0.09287377 -0.58931065 -0.7321844 ]\n",
      " [-0.30742547 -1.3045527  -0.05227635]]\n",
      "[[0.6285317  0.14024438 0.2312239 ]\n",
      " [0.04622407 0.11369288 0.84008306]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "logits = [[2,0.5,1],[0.1,1,3]]\n",
    "labels = [[0.2,0.3,0.5],[0.1,0.6,0.3]]\n",
    "logits_scaled = tf.nn.softmax(logits)\n",
    "\n",
    "result1 = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels)\n",
    "result2 = -tf.reduce_sum(labels*tf.log(logits_scaled),1)\n",
    "result3 = tf.nn.softmax_cross_entropy_with_logits(logits=logits_scaled,labels=labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('直接使用API计算softmax交叉熵：')\n",
    "    print(sess.run(result1))\n",
    "    print('\\n')\n",
    "    print('利用原理计算softmax交叉熵：')\n",
    "    print(sess.run(result2))\n",
    "    print('\\n')\n",
    "    print(sess.run(labels*tf.log(logits_scaled)))\n",
    "    print(sess.run(tf.nn.softmax(logits)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/chening/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-158-1dea64e6d8f5>:23: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-158-1dea64e6d8f5>:37: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "[0.02738266 0.36066383 0.08493882 0.04741147 1.3124981 ]\n",
      "[0.51290035 1.7430954  1.4329239  1.9272339  0.5157414 ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "Tensorflow实现何凯明的Focal Loss, 该损失函数主要用于解决分类问题中的类别不平衡\n",
    "focal_loss_sigmoid: 二分类loss\n",
    "focal_loss_softmax: 多分类loss\n",
    "Reference Paper : Focal Loss for Dense Object Detection\n",
    "\"\"\"\n",
    "\n",
    "def focal_loss_sigmoid(labels,logits,alpha=0.25,gamma=2):\n",
    "    \"\"\"\n",
    "    Computer focal loss for binary classification\n",
    "    Args:\n",
    "      labels: A int32 tensor of shape [batch_size].\n",
    "      logits: A float32 tensor of shape [batch_size].\n",
    "      alpha: A scalar for focal loss alpha hyper-parameter. If positive samples number\n",
    "      > negtive samples number, alpha < 0.5 and vice versa.\n",
    "      gamma: A scalar for focal loss gamma hyper-parameter.\n",
    "    Returns:\n",
    "      A tensor of the same shape as `lables`\n",
    "    \"\"\"\n",
    "    y_pred=tf.nn.sigmoid(logits)\n",
    "    labels=tf.to_float(labels)\n",
    "    L=-labels*(1-alpha)*((1-y_pred)*gamma)*tf.log(y_pred)-\\\n",
    "      (1-labels)*alpha*(y_pred**gamma)*tf.log(1-y_pred)\n",
    "    return L\n",
    "\n",
    "def focal_loss_softmax(labels,logits,gamma=2):\n",
    "    \"\"\"\n",
    "    Computer focal loss for multi classification\n",
    "    Args:\n",
    "      labels: A int32 tensor of shape [batch_size].\n",
    "      logits: A float32 tensor of shape [batch_size,num_classes].\n",
    "      gamma: A scalar for focal loss gamma hyper-parameter.\n",
    "    Returns:\n",
    "      A tensor of the same shape as `lables`\n",
    "    \"\"\"\n",
    "    y_pred=tf.nn.softmax(logits,dim=-1) # [batch_size,num_classes]\n",
    "    labels=tf.one_hot(labels,depth=y_pred.shape[1])\n",
    "    L=-labels*((1-y_pred)**gamma)*tf.log(y_pred)\n",
    "    L=tf.reduce_sum(L,axis=1)\n",
    "    return L\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logits=tf.random_uniform(shape=[5],minval=-1,maxval=1,dtype=tf.float32)\n",
    "    labels=tf.Variable([0,1,0,0,1])\n",
    "    loss1=focal_loss_sigmoid(labels=labels,logits=logits)\n",
    "\n",
    "    logits2=tf.random_uniform(shape=[5,4],minval=-1,maxval=1,dtype=tf.float32)\n",
    "    labels2=tf.Variable([1,0,2,3,1])\n",
    "    loss2=focal_loss_softmax(labels==labels2,logits=logits2)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run(loss1))\n",
    "        print(sess.run(loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits2=tf.random_uniform(shape=[5,4],minval=-1,maxval=1,dtype=tf.float32)\n",
    "labels2=tf.Variable([1,0,2,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=tf.nn.softmax(logits2,dim=-1) # [batch_size,num_classes]\n",
    "labels=tf.one_hot(labels2,depth=y_pred.shape[1])\n",
    "L=-labels*((1-y_pred)**gamma)*tf.log(y_pred)\n",
    "L=tf.reduce_sum(L,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88239956 0.86862177 0.49154782 0.7574308 ]\n",
      " [0.8032028  0.75464344 0.61760783 0.8245459 ]\n",
      " [0.7089582  0.8742371  0.90322185 0.5135828 ]\n",
      " [0.71932155 0.7609165  0.8038691  0.7158928 ]\n",
      " [0.79343116 0.8108505  0.6813462  0.71437216]]\n",
      "[[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "[0.2274034  2.0928283  0.5307078  0.589462   0.32456163]\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(1-y_pred))\n",
    "    print(sess.run(labels))\n",
    "    print(sess.run(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
